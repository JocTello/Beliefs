{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Identification of Investor Beliefs\n",
    "\n",
    "by [Xiaohong Chen](https://economics.yale.edu/people/faculty/xiaohong-chen), [Lars Peter Hansen](http://larspeterhansen.org/) and [Peter G. Hansen](https://mitsloan.mit.edu/phd/students/peter-hansen).\n",
    "\n",
    "The latest version of the paper can be found [here](http://larspeterhansen.org/research/papers/).\n",
    "\n",
    "Notebook by: Han Xu, Zhenhuan Xie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "This notebook provides the source code and explanations for how we solve the dynamic problem in Section 3 of the paper. It also provides the source code for the figures in Section 4 as well as additional results that we did not report in the paper. Before we describe and implement the computation, let's first install and load necessary `Python packages` (and set up the server environment if you are running this notebook on `Goolge Colab`) by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the notebook is open in Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set up Google Colab environment\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    # Link your Goolge Drive to Goolge Colab\n",
    "    drive.mount('/content/gdrive')\n",
    "    %cd '/content/gdrive/My Drive'\n",
    "    # Create a folder to store our project\n",
    "    if 'Belief_project' in os.listdir():\n",
    "        %cd '/content/gdrive/My Drive/Belief_project'\n",
    "        ! git pull\n",
    "    else:\n",
    "        ! mkdir '/content/gdrive/My Drive/Belief_project/'\n",
    "        %cd '/content/gdrive/My Drive/Belief_project/'\n",
    "    # Clone GitHub repo to the folder and change working directory to the repo\n",
    "    if 'Beliefs' not in os.listdir():\n",
    "        ! git clone https://github.com/lphansen/Beliefs.git\n",
    "    %cd '/content/gdrive/My Drive/Belief_project/Beliefs'\n",
    "\n",
    "# Set up local environment\n",
    "else:\n",
    "    try:\n",
    "        import plotly\n",
    "    except:\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install plotly\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from source.preprocessing import preprocess_data\n",
    "from source.solver import solve, find_ξ\n",
    "print('----------Successfully Loaded Python Packages----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Moment Bounds\n",
    "\n",
    "### 2.1 Relative Entropy Specification\n",
    "Recall **Problem 3.2** in the paper.  For a real number $\\mu$ and a random variable $v_0$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mu = \\min_{N_1 \\ge 0} \\mathbb{E}\\left(N_1\\left[g(X_1)+\\xi\\log N_1 + v_1\\right]\\mid \\mathfrak{I}_0\\right) - v_0\n",
    "\\end{equation}\n",
    "*subject to constraints*:\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[N_1 f(X_1)\\mid\\mathfrak{I}_0\\right] &= 0\\\\\n",
    "\\mathbb{E}\\left[N_1 \\mid \\mathfrak{I}_0\\right] &= 1\n",
    "\\end{align*}\n",
    "where $v_1$ is a version of $v_0$ shifted forward one time period.\n",
    "\n",
    "By **Proposition 3.8**, this problem can be solved by finding the solution to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\min_{\\lambda_0}\\mathbb E \\left(\\exp \\left[-\\frac{1}{\\xi}g(X_1)+\\lambda_0\\cdot f(X_1)\\right]\\left( \\frac{e_1}{e_0}\\right) \\mid \\mathfrak{I}_0\\right)\n",
    "\\end{equation}\n",
    "\n",
    "*where*\n",
    "\\begin{align*}\n",
    "\\mu &= -\\xi \\log \\epsilon,\\\\\n",
    "v_0 &= -\\xi \\log e_0.\n",
    "\\end{align*}\n",
    "\n",
    "The optimized results will depend on the choice of $\\xi$. Alternative values of $\\xi$ imply alternative bounds on the expectation of $g(X_1)$ and the corresponding relative entropy.  Below is an illustration of how the minimized objectives $\\mu^*$ and $\\epsilon^*$ change with $\\xi$. Data and calculation details are described later in Section 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from source.plots import objective_vs_ξ\n",
    "time_start = time.time()\n",
    "objective_vs_ξ(n_states=3) # Here we use relative entropy divergence\n",
    "print('Time spent:', round(time.time()-time_start,2),'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implied solution for the probablity distortion is:\n",
    "\n",
    "\\begin{equation}\n",
    "N_1^* = \\frac{\\exp \\left[-\\frac{1}{\\xi}g(X_1)+\\lambda^*_0(Z_0)\\cdot f(X_1)\\right]e_1^*}{\\epsilon^*e_0^*}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda^*_0$ is the optimizing choice for $\\lambda_0$ and $\\left(\\epsilon^*,e_0^*\\right)$ are selected so that the resulting $\\sf Q$ induces stochastically stable. The conditional expectation implied by the bound is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}\\left[N_1^*g(X_1)\\mid \\mathfrak{I}_0\\right]\n",
    "\\end{equation}\n",
    "\n",
    "which in turn implies a bound on the unconditional expectation equal to\n",
    "\n",
    "\\begin{equation}\n",
    "\\int \\mathbb{E}\\left[N_1^*g(X_1)\\mid\\mathfrak{I}_0\\right]d \\sf Q_0^*\n",
    "\\end{equation}\n",
    "\n",
    "The implied relative entropy is\n",
    "\n",
    "\\begin{equation}\n",
    "\\int \\mathbb{E}\\left(N_1^*\\log N_1^*\\mid \\mathfrak{I}_0\\right)d \\sf Q_0^*\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Quadratic Specification\n",
    "\n",
    "Intead of using relative entropy as the divergence measure, we can also use quadratic specification as discussed in the appendix of the paper. For a real number $\\mu$ and a random variable $v_0$,\n",
    "\\begin{equation}\n",
    "\\mu = \\min_{N_1\\geq 0}\\mathbb E \\left(N_1 \\left[g(X_1)+v_1\\right]+\\frac{\\xi}{2}(N_1^2-N_1) \\mid \\mathfrak{I}_0\\right) - v_0\n",
    "\\end{equation}\n",
    "\n",
    "*subject to constraints*:\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}\\left[N_1f(X_1)\\mid\\mathfrak{I}_0\\right] = 0\\\\\n",
    "&\\mathbb{E}\\left[N_1\\mid\\mathfrak{I}_0\\right] = 1\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, the problem can be solved by finding the solution to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu = \\max_{\\lambda_1,\\lambda_2} -\\frac{\\xi}{2}\\mathbb{E}\\left[\\left(\\left[\\frac{1}{2}-\\frac{1}{\\xi}\\left[g(X_1)+v_1+\\lambda_1 \\cdot f(X_1) + \\lambda_2\\right]\\right]^+\\right)^2\\mid\\mathfrak{I}_0\\right]-\\lambda_2-v_0\n",
    "\\end{equation}\n",
    "\n",
    "The implied solution for the probablity distortion is:\n",
    "\n",
    "\\begin{equation}\n",
    "N_1^* = \\left[\\frac{1}{2} - \\frac{1}{\\xi^*}\\left[g(X_1)+v_1^*+\\lambda_1^*\\cdot f(X_1)+\\lambda_2^*\\right]\\right]^+\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Chernoff Entropy\n",
    "\n",
    "In addition, we show how to calculate the Chernoff entropy mentioned in the paper. Suppose $P$ and $\\tilde{P}$ are transition probability matrices of two Markov processes.\n",
    "\n",
    "- Fix 0<s<1. Calculate the matrix $H_s\\left(P,\\tilde{P}\\right)$:\n",
    "$$\n",
    "H_s\\left(P,\\tilde{P}\\right)_{ij} = [P_{ij}]^s [\\tilde{P}_{ij}]^{1-s}\n",
    "$$ \n",
    "for $1\\leq i,j \\leq 741$.\n",
    "\n",
    "\n",
    "- Calculate the spectral radius of $H_s\\left(P,\\tilde{P}\\right)$:\n",
    "$$\n",
    "r = \\max_{1\\leq i\\leq 741} \\left\\{|\\lambda_i|\\right\\}\n",
    "$$\n",
    "where $\\{\\lambda_i\\}$ are the (possibly complex) eigenvalues for $H_s\\left(P,\\tilde{P}\\right)$.\n",
    "\n",
    "\n",
    "- Minimize $r$ with respect to $s$. Denote the minimized $r$ as $r^*$. Then $1-r^*$ is the Chernoff entropy.\n",
    "\n",
    "The Chernoff measure is motivated by a common decay rate imposed on type I and type II errors of testing one model against another and is expected to be considerably smaller. We computed it using the approach described in Newman and Stuck (1979) for Markov processes. While symmetric, this measure is less tractable to implement and not included in the family of recursive divergences that we describe. We use it merely to provide, ex post, additional information about the magnitude of the bound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the bounds on the expected logarithmic return on market, we let the logarithm of this return on wealth be our $g$; \n",
    "\n",
    "To compute the bounds on risk premium and generalized volatility, we extend the previous approach as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Bounding Risk Premia\n",
    "\n",
    "- Set $g(X_1)=R^w_1-\\zeta R^f_1$ where $\\zeta$ is a \"multiplier\" that we will search over;\n",
    "\n",
    "\n",
    "- for alternative $\\zeta$, deduce $N_1^*(\\zeta)$ and $\\sf Q_0^*(\\zeta)$ as described in the paper;\n",
    "\n",
    "\n",
    "- compute:\n",
    "\n",
    "$$\n",
    "\\log \\int \\mathbb{E}\\left[N_1^*(\\zeta)R^w_1\\mid \\mathfrak{I}_0\\right]d \\sf Q_0^*(\\zeta) - \\log \\int \\mathbb{E}\\left[N_1^*(\\zeta)R^f_1\\mid \\mathfrak{I}_0\\right]d \\sf Q_0^*(\\zeta)\n",
    "$$\n",
    "and minimize with respect to $\\zeta$;\n",
    "\n",
    "\n",
    "- set $g(X_1)=-R^w_1+\\zeta R^f_1$, repeat, and use the negative of the minimizer to obtain the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Bounding Volatility\n",
    "\n",
    "We show how to bound an entropic measure of volatility.  Other measures could be computed using a similar approch.  \n",
    "\n",
    "- Set $g(X_1)=R^w_1-\\zeta \\log R^w_1$ where $\\zeta$ is a \"multiplier\" that we will search over;\n",
    "\n",
    "\n",
    "- for alternative $\\zeta$, deduce $N_1^*(\\zeta)$ and $\\sf Q_0^*(\\zeta)$ as described in the paper;\n",
    "\n",
    "\n",
    "- compute:\n",
    "\n",
    "$$\n",
    "\\log \\int \\mathbb{E}\\left[N_1^*(\\zeta)R^w_1\\mid \\mathfrak{I}_0\\right]d {\\sf Q_0}^*(\\zeta) - \\int \\mathbb{E}\\left[N_1^*(\\zeta)\\log R^w_1\\mid \\mathfrak{I}_0\\right]d {\\sf Q_0}^*(\\zeta)\n",
    "$$\n",
    "and minimize with respect to $\\zeta$;\n",
    "\n",
    "\n",
    "- set $g(X_1)=-R^w_1+\\zeta \\log R^w_1$, repeat, and use the negative of the minimizer to obtain the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data\n",
    "The file “UnitaryData.csv” contains the following data from 1954-2016:\n",
    "\n",
    "- The first four columns contain Euler equation errors from the unitary risk aversion model corresponding to the 3-month T-bill rate, the market excess return, the SMB excess return, and the HML excess return respectively. Under a feasible belief distortion, all four of these variables should have expectation of zero (conditional or unconditional).\n",
    "\n",
    "\n",
    "- The column “d.p” contains the dividend-price ratio for the CRSP value-weighted index, computed at the start of the return period. Hence functions of d.p[i] (i.e. quantile indicator functions) are valid instruments for the returns in row i.\n",
    "\n",
    "\n",
    "- The final column “log.RW” contains values of the logarithmic return on CRSP value-weighted index. We use this as a proxy for the logarithmic return on wealth. This is the random variable whose expectation we are intersted in bounding.\n",
    "\n",
    "All returns are quarterly and inflation-adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('data/UnitaryData.csv')\n",
    "# Show statistics of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our direct use of dividend-price measures, we purposefully choose a coarse conditioning of information and split the dividend price ratios into $n$ bins using the empirical percentiles. We take the dividend-price percentiles to be a $n$-state Markov process. Then we multiply each of the first four columns by each of the $n$ columns of the indicator function of dividend-price percentiles to form a $4n$-dimensional $f$. \n",
    "\n",
    "When bounding the expected logarithmic return on wealth, we take $\\log R^w$ as our $g$. When bounding the risk premium and volatility, we define $g$ as discussed above in Section 2.4 and 2.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computational Strategy\n",
    "\n",
    "Since we have $n$ distinct states in our application, we can represent the function $e(\\cdot)$ as a $n$-dimensional vector. Additionally, we are free to impose the normalization $e_1=1$. We can solve the dual problem numerically by something analogous to value function iteration for $e=(1,e_2,...,e_n)$. Here is the iteration scheme:\n",
    "\n",
    "\n",
    "1\\. Guess $e={\\mathbb{1}}_{n\\times 1}$.\n",
    "\n",
    "2\\. For $k \\in \\{1,2,...,n\\}$, solve\n",
    "\\begin{equation}\n",
    "v_k = \\min_{\\lambda_0} \\hat{\\mathbb{E}}\\left(\\exp \\left[-\\frac{1}{\\xi}g(X_1) + \\lambda_0 \\cdot f(X_1)\\right]e(Z_0)\\mid Z_0 = k\\right)\n",
    "\\end{equation}\n",
    "\n",
    "3\\. Store\n",
    "\\begin{align*}\n",
    "\\hat{e} &= v/v_1 \\\\\n",
    "\\hat{\\epsilon} &= v_1 \\\\\n",
    "\\text{error} &= \\|\\hat{e}-e\\|\n",
    "\\end{align*}\n",
    "\n",
    "4\\. Set $e = \\hat{e}$.\n",
    "\n",
    "5\\. Iterate steps 2-4 until error is smaller than $10^{-9}$.\n",
    "\n",
    "Once we have (approximately) stationary values for $\\epsilon^*$ and $e^*$ as well as the optimizing $\\lambda_0^*$, we can form the conditional belief distortion\n",
    "\\begin{equation}\n",
    "N_1 = \\frac{1}{\\epsilon^*} \\exp \\left[-\\frac{1}{\\xi}g(X_1)+\\lambda_0^* \\cdot f(X_1)\\right]\\frac{e^*(Z_1)}{e^*(Z_0)}\n",
    "\\end{equation}\n",
    "\n",
    "To obtain the unconditional relative entropy, we need to average across states using the implied stationary distribution coming from the distorted probabilities. Define a $n\\times n$ matrix $\\tilde{P}$ by \n",
    "$$\n",
    "\\tilde{P}_{i,j} = \\hat{\\mathbb{E}}\\left[N_1 \\mathcal{1}\\left(Z_1 = j\\right)\\mid Z_0 = i\\right]\n",
    "$$\n",
    "\n",
    "We should have that $\\tilde{P}$ is a transition probability matrix, so $\\tilde{P}\\mathbb{1}=\\mathbb{1}$. Next, solve for the stationary distribution $\\pi\\in \\mathbb{R}^n$ as the dominant left eigenvector of $\\tilde{P}$, i.e.\n",
    "\\begin{equation}\n",
    "\\tilde{\\pi}^\\prime \\tilde{P} = \\tilde{\\pi}^\\prime\n",
    "\\end{equation}\n",
    "\n",
    "Then, the unconditional relative entropy can be computed as\n",
    "\\begin{equation}\n",
    "\\text{RE}(\\xi) = \\sum_{k=1}^{n}\\hat{\\mathbb{E}}\\left[N_1\\log N_1 \\mid Z_0 = k\\right]\\cdot \\tilde{\\pi}_k\n",
    "\\end{equation}\n",
    "\n",
    "Note 1: the implementation is assuming a relative entropy divergence, but the iteration scheme also works for the quadratic specification of divergence.\n",
    "\n",
    "Note 2: in the following code implementation, we set $n=3$ as used in the paper. Users can specify a different $n$ by changing the `n_states` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import print_results\n",
    "time_start = time.time()\n",
    "\n",
    "n_states = 3 # User can set to other positive integers\n",
    "quadratic = False # User can set to True to use quadratic divergence\n",
    "\n",
    "div = 'QD' if quadratic else 'RE'\n",
    "f, log_Rw, z0, z1, Rf, Rm, SMB, HML = preprocess_data(n_states)\n",
    "g = log_Rw # Set g to be log return on wealth\n",
    "\n",
    "# Minimum divergence case\n",
    "result_min = solve(f, g, z0, z1, ξ=10., quadratic=quadratic,\n",
    "                   tol=1e-9, max_iter=1000)\n",
    "\n",
    "# 20% higher divergence case, lower bound problem\n",
    "ξ_20_lower = find_ξ(solver_args=(f, g, z0, z1, quadratic, 1e-9, 1000),\n",
    "                    min_div=result_min[div], pct=0.2, initial_guess=1.,\n",
    "                    interval=(0, 10.), tol=1e-5, max_iter=100)\n",
    "result_lower = solve(f, g, z0, z1, ξ=ξ_20_lower, quadratic=quadratic,\n",
    "                     tol=1e-9, max_iter=1000)\n",
    "\n",
    "# 20% higher divergence case, upper bound problem\n",
    "ξ_20_upper = find_ξ(solver_args=(f, -g, z0, z1, quadratic, 1e-9, 1000),\n",
    "                    min_div=result_min[div], pct=0.2, initial_guess=1.,\n",
    "                    interval=(0, 10.), tol=1e-5, max_iter=100)\n",
    "result_upper = solve(f, -g, z0, z1, ξ=ξ_20_upper, quadratic=quadratic,\n",
    "                     tol=1e-9, max_iter=1000)\n",
    "\n",
    "print_results(result_lower, result_upper, quadratic)\n",
    "\n",
    "print('Time spent:', round(time.time()-time_start,2),'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utilities import construct_transition_matrix, chernoff_entropy\n",
    "time_start = time.time()\n",
    "# Compute Chernoff entropy for empirical distribution and distorted distribution with min rel entropy\n",
    "P_big, P_big_tilde = construct_transition_matrix(f, g, z0, z1, result_min['ξ'],\n",
    "                                                 result_min['P'], result_min['P_tilde'],\n",
    "                                                 result_min['λ'], result_min['v'],\n",
    "                                                 result_min['μ'], quadratic)\n",
    "decay_rate, optimal_s = chernoff_entropy(P_big, P_big_tilde, grid_size=1000)\n",
    "print('Chernoff entropy at the minimum: ', np.around(decay_rate,4))\n",
    "print('Time spent:', round(time.time()-time_start,2),'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import entropy_moment_bounds\n",
    "time_start = time.time() \n",
    "entropy_moment_bounds(n_states) # Here we use relative entropy divergence\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tables and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import figure_1\n",
    "time_start = time.time()\n",
    "print('Figure 1: Expected log market return')\n",
    "figure_1()\n",
    "print('Note 1: here we use n_states = 3 and a relative entropy divergence.')\n",
    "print('Note 2: user can control the slider to change the percent increase')\n",
    "print('        of relative entropy from minimum.')\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import figure_2\n",
    "time_start = time.time()\n",
    "print(\"Figure 2: Proportional risk compensations\")\n",
    "figure_2()\n",
    "print('Note 1: here we use n_states = 3 and a relative entropy divergence.')\n",
    "print('Note 2: user can control the slider to change the percent increase')\n",
    "print('        of relative entropy from minimum.')\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import table_1\n",
    "time_start = time.time()\n",
    "table_1(n_states=3, quadratic=False)\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import table_2\n",
    "time_start = time.time()\n",
    "table_2()\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import table_3\n",
    "time_start = time.time()\n",
    "table_3()\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import table_4\n",
    "time_start = time.time()\n",
    "table_4()\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.plots import table_5\n",
    "time_start = time.time()\n",
    "table_5(n_states=3)\n",
    "print(\"Time spent: %s seconds ---\" % (round(time.time()-time_start,4)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
