{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit,float64\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('UnitaryData.csv')\n",
    "pd_lag = np.array(data['d.p'])\n",
    "\n",
    "# Calculate terciles for pd ratio\n",
    "tercile_1 = np.quantile(pd_lag,1./3)\n",
    "tercile_2 = np.quantile(pd_lag,2./3)\n",
    "\n",
    "# Calculate indicator based on today's pd ratio\n",
    "pd_lag_indicator = np.array([pd_lag <= tercile_1,(pd_lag <= tercile_2) & (pd_lag > tercile_1),pd_lag > tercile_2]).T\n",
    "\n",
    "# Calculate indicator for tomorrow's pd ratio\n",
    "pd_indicator = pd_lag_indicator[1:]\n",
    "\n",
    "# Drop last row since we do not have tomorrow's pd ratio at that point\n",
    "pd_lag_indicator = pd_lag_indicator[:-1]\n",
    "X = np.array(data[['Rf','Rm-Rf','SMB','HML']])[:-1]\n",
    "f = np.hstack((X * pd_lag_indicator[:,:1],X * pd_lag_indicator[:,1:2],X * pd_lag_indicator[:,2:3]))\n",
    "g = np.array(data['log.RW'])[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Incorporating Conditional Information\n",
    "\n",
    "### 7.1 Approach I\n",
    "\\begin{equation}\n",
    "\\min_{M\\geq0}{\\mathbb E}\\left[Mg(X)\\right]\n",
    "\\end{equation}\n",
    "*subject to constraints:*\n",
    "\\begin{align*}\n",
    "&{\\mathbb E}\\left[M \\log M\\right] \\leq \\kappa, \\\\\n",
    "&{\\mathbb E}\\left[B^j M f(X)\\right] = 0, \\\\\n",
    "&{\\mathbb E}\\left[M\\right] = 1.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Approach II\n",
    "\\begin{equation}\n",
    "\\min_{M\\geq0}{\\mathbb E}\\left[Mg(X)\\right]\n",
    "\\end{equation}\n",
    "*subject to constraints:*\n",
    "\\begin{align*}\n",
    "&{\\mathbb E}\\left[M \\log M\\right] \\leq \\kappa, \\\\\n",
    "&{\\mathbb E}\\left[B^j M {f(X)}\\right] = 0, \\\\\n",
    "&{\\mathbb E}\\left[B^j M-B^j\\right] = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Approach III\n",
    "\n",
    "#### Basic problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{M\\geq0}{\\mathbb E}\\left[Mg(X)\\mid\\mathfrak{J}\\right]\n",
    "\\end{equation}\n",
    "*subject to constraints:*\n",
    "\\begin{align*}\n",
    "&{\\mathbb E}\\left[M \\log M\\mid\\mathfrak{J}\\right] \\leq \\kappa, \\\\\n",
    "&{\\mathbb E}\\left[M  f(X)\\mid\\mathfrak{J}\\right] = 0, \\\\\n",
    "&{\\mathbb E}\\left[M\\mid\\mathfrak{J}\\right] = 1.\n",
    "\\end{align*}\n",
    "\n",
    "#### Dual problem:\n",
    "For computational purposes, we solve the dual problem after minimizing over $M$.  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\max_{\\xi \\ge 0, \\hat{\\lambda}}    - \\xi \\log {\\mathbb E} \\left[ \\exp\\left( - {\\frac 1 {\\xi}} \\left[ g(X) + \\hat{\\lambda} \\cdot f(X) \\right] \\right)\\mid\\mathfrak{F}\\right]  -  \\xi \\kappa  \n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{\\lambda}$ and $\\xi$ are multipliers on the moment condition and relative entropy constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Intertemporal Divergence Constraints\n",
    "\n",
    "### Proposition 8.6\n",
    "Problem 8.4 can be solved by finding the solution to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\min_\\hat{\\lambda}\\mathbb E \\left(\\exp \\left[-\\frac{1}{\\xi}g(X_1)+\\hat{\\lambda}\\cdot f(X_1)\\right]\\left( \\frac{e_1}{e_0}\\right) \\mid \\mathfrak{F}_0\\right)\n",
    "\\end{equation}\n",
    "\n",
    "*where*\n",
    "\\begin{align*}\n",
    "\\mu &= -\\xi \\log \\epsilon,\\\\\n",
    "v_0 &= -\\xi \\log e_0.\n",
    "\\end{align*}\n",
    "\n",
    "Denote $e_0^*$, $e_1^*$, $\\hat{\\lambda}^*$ as the solution to the above optimization problem. The implied solution for the probablity distortion is:\n",
    "\n",
    "\\begin{equation}\n",
    "M_1^* = \\frac{\\exp \\left[-\\frac{1}{\\xi}g(X_1)+\\hat{\\lambda}^*(Z_0)\\cdot f(X_1)\\right]e_1^*}{\\epsilon^*e_0^*}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(λ):\n",
    "    selector = pd_lag_indicator[:,state-1]\n",
    "    term_1 = -g[selector]/ξ\n",
    "    term_2 = f[:,(state-1)*4:state*4][selector]@λ\n",
    "    term_3 = np.log(pd_indicator[selector]@e)\n",
    "    x = term_1 + term_2 + term_3\n",
    "    # use \"max trick\" to improve accuracy\n",
    "    a = x.max()\n",
    "    # log_E_exp(x)\n",
    "    return np.log(np.sum(np.exp(x-a))) + a\n",
    "\n",
    "def min_objective():\n",
    "    model = minimize(objective, \n",
    "                     np.ones(4), \n",
    "                     method='L-BFGS-B',\n",
    "                     tol=1e-10,\n",
    "                     options={'maxiter': 1000})\n",
    "    v = np.exp(model.fun)/np.sum(pd_lag_indicator[:,state-1])\n",
    "    λ = model.x\n",
    "    return v,λ\n",
    "\n",
    "def iteration():\n",
    "    # set global variables\n",
    "    global state\n",
    "    global e\n",
    "    \n",
    "    # initial error\n",
    "    error = 1.\n",
    "    # count times\n",
    "    count = 0\n",
    "\n",
    "    while error > 1e-10:\n",
    "        if count == 0:\n",
    "            # initial guess for e\n",
    "            e = np.array([1,1,1])\n",
    "            # placeholder for v\n",
    "            v = np.zeros(3)   \n",
    "            # placeholder for λ\n",
    "            λ = np.zeros(12)\n",
    "        for k in [1,2,3]:\n",
    "            state = k\n",
    "            v[state-1],λ[(state-1)*4:state*4] = min_objective()\n",
    "        # update e and ϵ\n",
    "        e_old = e\n",
    "        ϵ = v[0]\n",
    "        e = v/v[0]\n",
    "        error = np.max(np.abs(e - e_old))\n",
    "        count += 1\n",
    "        \n",
    "    return ϵ,e,λ,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.7294 seconds ---\n",
      "--- 235 iterations ---\n",
      "--- ϵ: 0.9662646500347174 ---\n",
      "--- e: [1.      0.44192 0.19224] ---\n",
      "--- λ: [ 1.20466  0.79534 -0.31077 -0.76063  1.42673  0.5733   0.66143 -5.9703\n",
      "  2.68479 -0.68482 -3.4839  -7.80325] ---\n"
     ]
    }
   ],
   "source": [
    "ξ = 1.\n",
    "\n",
    "time_start = time.time() \n",
    "ϵ,e,λ,count = iteration()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))\n",
    "print(\"--- %s iterations ---\" % count)\n",
    "print(\"--- ϵ: %s ---\" % ϵ)\n",
    "print(\"--- e: %s ---\" % e)\n",
    "print(\"--- λ: %s ---\" % λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[M|state 1] = 0.9999999999954226 \n",
      "E[M|state 2] = 0.9999999997898478 \n",
      "E[M|state 3] = 0.9999999996547466 \n"
     ]
    }
   ],
   "source": [
    "# Calculate M\n",
    "M = 1./ϵ * np.exp(-g/ξ+f@λ) * (pd_indicator@e) / (pd_lag_indicator@e)\n",
    "\n",
    "# Check 1: E[M|state k] = 1\n",
    "print(\"E[M|state 1] = %s \" % np.mean(M[pd_lag_indicator[:,0]]))\n",
    "print(\"E[M|state 2] = %s \" % np.mean(M[pd_lag_indicator[:,1]]))\n",
    "print(\"E[M|state 3] = %s \" % np.mean(M[pd_lag_indicator[:,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[MlogM|state 1] = 0.012404221512381448 \n",
      "E[MlogM|state 2] = 0.06706655850983517 \n",
      "E[MlogM|state 3] = 0.16819195810069526 \n"
     ]
    }
   ],
   "source": [
    "# Calculate  conditional relative entropy\n",
    "RE_1 = np.mean(M[pd_lag_indicator[:,0]]*np.log(M[pd_lag_indicator[:,0]]))\n",
    "RE_2 = np.mean(M[pd_lag_indicator[:,1]]*np.log(M[pd_lag_indicator[:,1]]))\n",
    "RE_3 = np.mean(M[pd_lag_indicator[:,2]]*np.log(M[pd_lag_indicator[:,2]]))\n",
    "\n",
    "# Print conditional relative entropy\n",
    "print(\"E[MlogM|state 1] = %s \" % RE_1)\n",
    "print(\"E[MlogM|state 2] = %s \" % RE_2)\n",
    "print(\"E[MlogM|state 3] = %s \" % RE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_tilde:\n",
      " [[0.97955 0.02045 0.     ]\n",
      " [0.08402 0.88102 0.03496]\n",
      " [0.      0.17515 0.82485]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate transition matrix under the distorted belief\n",
    "P = np.zeros((3,3))\n",
    "for i in [1,2,3]:\n",
    "    for j in [1,2,3]:\n",
    "        P[i-1,j-1] = np.mean(M[pd_lag_indicator[:,i-1]]*pd_indicator[pd_lag_indicator[:,i-1]][:,j-1])\n",
    "# Print out the transition matrix\n",
    "print(\"P_tilde:\\n\", P)\n",
    "# P@np.ones(3).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π = [0.77399 0.1884  0.03761]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the stationary distribution\n",
    "A = P.T - np.eye(3)\n",
    "A[-1] = np.ones(3)\n",
    "B = np.zeros(3)\n",
    "B[-1] = 1.\n",
    "π = np.linalg.solve(A, B)\n",
    "# π_check = np.linalg.matrix_power(P, 500)\n",
    "print(\"π = %s\" % π)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[MlogM] = 0.028561418675555502\n"
     ]
    }
   ],
   "source": [
    "# Calculate unconditional relative entropy\n",
    "RE = np.array([RE_1,RE_2,RE_3]) @ π\n",
    "print(\"E[MlogM] = %s\" % RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[Mg(X)] = 0.005756098776325266\n"
     ]
    }
   ],
   "source": [
    "# Implied moment bound, 1st approach: use \\mu-\\xi*RE\n",
    "moment_bound = - 1./ξ * np.log(ϵ) - ξ*RE\n",
    "\n",
    "# # Implied moment bound, 2nd approach: use E[Mg(X)|state]π\n",
    "# Mg_1 = np.mean(M[pd_lag_indicator[:,0]]*g[pd_lag_indicator[:,0]])\n",
    "# Mg_2 = np.mean(M[pd_lag_indicator[:,1]]*g[pd_lag_indicator[:,1]])\n",
    "# Mg_3 = np.mean(M[pd_lag_indicator[:,2]]*g[pd_lag_indicator[:,2]])\n",
    "# moment_bound_check = np.array([Mg_1,Mg_2,Mg_3]) @ π\n",
    "\n",
    "print(\"E[Mg(X)] = %s\" % moment_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
