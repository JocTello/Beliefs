{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Sets\n",
    "\n",
    "In this notebook we provide an estimation method for the confidence intervals of the lower/upper bounds in CHH(2020). We follow the estimation framework in CCT(2018).\n",
    "\n",
    "## 1. Moments\n",
    "\n",
    "To simplify notation we let $\\lambda(z) = (\\lambda_1(z), ... , \\lambda_d(z), \\nu(z))^\\prime$ and $\\lambda_{d+1}(z) = \\nu(z)$. Denote $\\alpha=(\\mu, \\lambda, v)$ and $\\alpha^*=(\\mu^*,\\lambda^*, v^*)$. Let \n",
    "\n",
    "$$\n",
    "\\rho(X_1, \\alpha, \\theta) = \\begin{bmatrix} f(X_1, \\theta) N_1(\\lambda_0, v_1) \\\\ N_1(\\lambda_0, v_1)-1 \\\\ F(X_1, \\lambda_0, v_1, \\theta) -\\mu\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "F(X_1, \\lambda_0, v_1, \\theta) = -\\xi\\exp(\\lambda_0\\cdot f(X_1) + \\nu_0 - \\frac{1}{\\xi}(g(X_1)+v(Z_1)) - 1) - v(Z_0) + \\xi\\nu_0\n",
    "$$\n",
    "\n",
    "For relative entropy divergence the problem slightly simplifies as $\\nu^*$ could be solved out in other parameters. Let $\\lambda=(\\lambda_1, ..., \\lambda_d)^\\prime.$ Let $\\alpha=(\\mu, \\lambda, v)$ and $\\alpha^*=(\\mu^*, \\lambda^*, v^*)$. Let\n",
    "\n",
    "$$\n",
    "\\rho(X_1, \\alpha, \\theta) = \\begin{bmatrix}f(X_1, \\theta)\\exp\\left[-\\frac{g(X_1,\\theta)+v(Z_1)}{\\xi} + \\lambda(Z_0)\\cdot f(X_1,\\theta)\\right] \\\\ \\exp\\left[-\\frac{g(X_1,\\theta)+v(Z_1)-v(Z_0)}{\\xi} + \\lambda(Z_0)\\cdot f(X_1,\\theta)\\right] - \\exp\\left[-\\frac{\\mu}{\\xi}\\right]\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\rho(X_1, \\alpha, \\theta) = \\begin{bmatrix}f(X_1, \\theta)\\exp\\left[-\\frac{g(X_1,\\theta)+v(Z_1)-v(Z_0)}{\\xi} + \\lambda(Z_0)\\cdot f(X_1,\\theta)\\right] \\\\ \\exp\\left[-\\frac{g(X_1,\\theta)+v(Z_1)-v(Z_0)}{\\xi} + \\lambda(Z_0)\\cdot f(X_1,\\theta)\\right] - \\exp\\left[-\\frac{\\mu}{\\xi}\\right]\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2. Profile Sieve Estimation With Fixed $\\theta$\n",
    "\n",
    "### 2.1 Optimally-weighted or continuously-updated sieve GMM estimation with fixed $\\theta$\n",
    "\n",
    "Following Hansen, Heaton and Yaron (1996) we define the continuously updated sieve GMM criterion function: $\\max_{\\alpha\\in \\mathcal{A}_t} L_T(\\alpha)$ with\n",
    "\n",
    "$$\n",
    "L_T(\\alpha) = -0.5\\left[\\frac{1}{T}\\sum_{t=1}^T P^{K_T}(Z_{t-1})\\otimes \\rho(X_t, \\alpha, \\theta)\\right]^\\prime \\left[\\hat{W}(\\alpha,\\theta)\\right]^- \\left[\\frac{1}{T}\\sum_{t=1}^T P^{K_T}(Z_{t-1})\\otimes \\rho (X_t, \\alpha, \\theta)\\right]\n",
    "$$\n",
    "\n",
    "where $P^{K_T}(Z_{t-1})$ is $K_T$-dimensional vector of known basis functions (such as splines), and for each $\\alpha \\in \\mathcal{A}_T$, $\\hat{W}(\\alpha,\\theta)$ is a consistent estimator of $W(\\alpha, \\theta)$:\n",
    "\n",
    "$$\n",
    "W(\\alpha, \\theta) = \\lim_T \\text{Var} \\left(\\frac{1}{\\sqrt{T}}\\sum_{t=1}^T P^{K_T}(Z_{t-1})\\otimes \\rho(X_t, \\alpha, \\theta)\\right)\n",
    "$$\n",
    "\n",
    "In particular,\n",
    "$$\n",
    "W(\\alpha^*, \\theta) = E[P^{K_T}(Z_{t-1})\\otimes \\Omega(Z_{t-1}, \\alpha^*, \\theta)\\otimes P^{K_T}(Z_{t-1})^\\prime]\n",
    "$$\n",
    "\n",
    "\n",
    "## 3. Monte Carlo Confidence Sets\n",
    "\n",
    "### 3.1 Confidence sets for the identified set $\\theta_I$\n",
    "\n",
    "Here we seek a $100\\alpha \\%$ CS $\\hat{\\Theta}_\\alpha$ for $\\Theta_I$ using $L_n(\\theta)$ that has asymptotically exact coverage, i.e.:\n",
    "\n",
    "$$\n",
    "\\lim_{n\\to \\infty} \\mathbb{P}(\\Theta_I \\subseteq \\hat{\\Theta}_\\alpha) = \\alpha\n",
    "$$\n",
    "\n",
    "1. Draw a sample $\\{\\theta^1,...,\\theta^B\\}$ from the quasi-posterior distribution $\\Pi_n := \\frac{\\exp(n L_n(\\theta)) \\Pi(\\theta)}{\\int_{\\Theta}\\exp(n L_n(\\theta)\\Pi(\\theta))}$\n",
    "\n",
    "2. Calculate the $(1-\\alpha)$ quantile of $\\{L_n(\\theta^1),...,L_n(\\theta^B)\\}$; call it $\\zeta_{n,\\alpha}^{mc}$.\n",
    "\n",
    "3. Our $100\\alpha\\%$ confidence set for $\\Theta_I$ is then:\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta}_\\alpha = \\{\\theta \\in \\Theta: L_n(\\theta)\\geq \\zeta_{n,\\alpha}^{mc}\\}\n",
    "$$\n",
    "\n",
    "### 3.2 Confidence sets for the identified set $M_I$ of subvectors\n",
    "\n",
    "Here we seek a $100\\alpha \\%$ CS $\\hat{M}_\\alpha$ for $M_I$ using $L_n(\\theta)$ that has asymptotically exact coverage, i.e.:\n",
    "\n",
    "$$\n",
    "\\lim_{n\\to \\infty} \\mathbb{P}(M_I \\subseteq \\hat{M}_\\alpha) = \\alpha\n",
    "$$\n",
    "\n",
    "#### 3.2.1 Projection\n",
    "A well-known method to construct a CS for $M_I$ is based on projection, which maps a CS $\\hat{\\Theta}_\\alpha$ for $\\Theta_I$ into one for $M_I$. The projection CS:\n",
    "\n",
    "$$\n",
    "\\hat{M}_\\alpha^{proj} = \\{\\mu: (\\mu, \\eta) \\in \\hat{\\Theta}_\\alpha \\text{ for some } \\eta \\}\n",
    "$$\n",
    "\n",
    "is a valid $100\\alpha\\%$ CS for $M_I$ whenever $\\hat{\\Theta}_\\alpha$ is a valid $100\\alpha\\%$ CS for $\\Theta_I$. As is well documented, $\\hat{M}_\\alpha^{proj}$ is typically conservative, and especially so when the dimension of $\\mu$ is small relative to the dimension of $\\theta$.\n",
    "\n",
    "#### 3.2.2 Profile criterion\n",
    "We propose CSs for $M_I$ based on a profile criterion for $M_I$. Let $M = \\{\\mu: (\\mu, \\eta) \\in \\Theta \\text{ for some } \\eta \\}$ and $H_\\mu = \\{\\eta: (\\mu, \\eta) \\in \\Theta\\}$. The profile criterion for a point $\\mu \\in M$ is $\\sup_{\\eta \\in H_\\mu}L_n(\\mu, \\eta)$, and the profile criterion for $M_I$ is\n",
    "\n",
    "$$\n",
    "PL_n(M_I) = \\inf_{\\mu \\in M_I} \\sup_{\\eta\\in H_\\mu}L_n(\\mu, \\eta)\n",
    "$$\n",
    "\n",
    "Let $\\Delta(\\theta^b)$ be an equivalence set for $\\theta^b$. In moment-based models we define $\\Delta(\\theta^b)=\\{\\theta\\in\\Theta:E[\\rho(X_i, \\theta)]=E[\\rho(X_i, \\theta^b)]\\}$. Let $M(\\theta^b)=\\{\\mu:(\\mu, \\eta)\\in \\Delta(\\theta^b) \\text{ for some }\\eta \\}$ and the profile criterion for $M(\\theta^b)$ is\n",
    "\n",
    "$$\n",
    "PL_n(M(\\theta^b)) = \\inf_{\\mu\\in M(\\theta^b)} \\sup_{\\eta\\in H_\\mu} L_n(\\mu, \\eta)\n",
    "$$\n",
    "\n",
    "1. Draw a sample $\\{\\theta^1, ..., \\theta^B\\}$ from the quasi-posterior distribution $\\Pi_n$.\n",
    "\n",
    "2. Calculate the $(1-\\alpha)$ quantile of $\\{PL_n(M(\\theta^b)): b=1,...,B\\}$; call it $\\zeta_{n,\\alpha}^{mc,p}$.\n",
    "\n",
    "3. Our $100\\alpha\\%$ confidence set for $M_I$ is then:\n",
    "$$\n",
    "\\hat{M}_\\alpha = \\{\\mu\\in M: \\sup_{\\eta\\in H_\\mu}L_n(\\mu, \\eta)\\geq \\zeta_{n,\\alpha}^{mc,p}\\}\n",
    "$$\n",
    "\n",
    "### 3.3 Adaptive Sequential Monte Carlo Algorithm\n",
    "\n",
    "Let $J$ and $K$ be positive integers and let $\\phi_1,...,\\phi_J$ be an increasing sequence with $\\phi_1=0$ and $\\phi_J=1$. Set $w_1^b=1$ for $b=1,...,B$ and draw $\\theta_1^1, ..., \\theta_1^B$ from the prior $\\Pi(\\theta)$. \n",
    "\n",
    "For $j=2,...,J$, do:\n",
    "\n",
    "1. Correction: Let $v_j^b = \\exp((\\phi_j-\\phi_{j-1})nL_n(\\theta^b_{j-1}))$ and $w_j^b = (v_j^bw_{j-1}^b)/(\\frac{1}{B}\\sum_{b=1}^Bv_j^bw_{j-1}^b)$.\n",
    "\n",
    "2. Selection: Compute the effective sample size $ESS_j = B/(\\frac{1}{B}\\sum_{b=1}^B(w_j^b)^2)$. Then:\n",
    "    - If $ESS_j>\\frac{B}{2}$: set $\\ell_j^b = \\theta_{j-1}^b$ for $b=1,...,B$; or\n",
    "    - If $ESS_j\\leq \\frac{B}{2}$: draw an i.i.d. sample $\\ell_j^1,...,\\ell_j^B$ from the multinomial distribution with support $\\theta_{j-1}^1,...,\\theta_{j-1}^B$ and weights $w_j^1,...,w_j^B$, then set $w_j^b=1$ for $b=1,...,B$.\n",
    "\n",
    "\n",
    "3. Mutation: Run $B$ separate and independent MCMC chains of length $K$ using the random-walk Metropolis-Hastings algorithm initialized at each $\\ell_j^b$ for the tempered quasi-posterior $\\Pi_j(\\theta|\\mathbf{X}_n)\\propto \\exp(\\phi_j n L_n(\\theta))\\Pi(\\theta)$ and let $\\theta_j^b$ be the final draw of the $b$th chain.\n",
    "\n",
    "The resulting sample is $\\theta^b = \\theta^b_J$ for $b=1,...,B$. Multinomial resampling (step 2) and the $B$ independent MCMC chains (step 3) can both be computed in parallel, so the additional computational time relative to conventional MCMC methods is modest.\n",
    "\n",
    "#### 3.3.1 Metropolis-Hastings\n",
    "\n",
    "Suppose $q$ is the proposal distribution, $\\pi$ is the desired joint distribution. \n",
    "\n",
    "Initialize $x_0$. for iteration $i = 1,2,...$ do:\n",
    "1. Propose: $x^{cand} \\sim q\\left(x^{(i)}|x^{(i-1)}\\right)$\n",
    "2. Acceptance Probability:\n",
    "    $$\n",
    "    \\alpha(x^{(cand)}|x^{(i-1)}) = \\min \\{1, \\frac{q\\left(x^{(i-1)}|x^{cand}\\right)\\pi(x^{cand})}{q\\left(x^{cand}|x^{(i-1)}\\right)\\pi(x^{(i-1)})}\\}\n",
    "    $$\n",
    "3. Draw $u$ from $\\text{Uniform }(0,1)$;\n",
    "4. If $u<\\alpha$, then accept the proposal: $x^{(i)} \\leftarrow x^{cand}$; Else, reject the proposal: $x^{(i)} \\leftarrow x^{(i-1)}$\n",
    "\n",
    "Note 1: In the applicaton, we use a random walk chain, i.e. $x^{(i)}=x^{(i-1)}+\\epsilon$, where $\\epsilon$ is a multivariate normal distribution with mean $0$ and covariance matrix $\\Sigma$.\n",
    "\n",
    "Note 2: We can initialize $x_0$ from an arbitrary distribution, for example, the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
