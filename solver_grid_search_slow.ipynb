{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Aim of the notebook\n",
    "\n",
    "\n",
    "This notebook provides Python code for an extension of GMM methods to accomomodate model misspecification.  It uses relative entropy discrepancy to accomodate and measure the magnitude of the misspecification.  The standard GMM problem starts with \n",
    "$$ \n",
    "{\\mathbb E} \\left[ f(X_t, \\theta) \\right] = 0\n",
    "$$\n",
    "Instead we start with\n",
    "$$\n",
    "{\\mathbb E} \\left[ M f(X, \\theta) \\right] = 0\n",
    "$$\n",
    "for $M \\ge 0$ and $E(M) = 1$. The random variable $M$ alters the probability measure.  We use $E( M \\log M)$ to measure divergence in the altered probability and hence as a measure of misspecification. To implement these methods, we replace population moments sample analogs.\n",
    "\n",
    "\n",
    "## Basic problem\n",
    "\n",
    "For $\\kappa > {\\underline \\kappa}$, a scalar function $h(X, \\theta)$ and a set of admissible parameters $\\Theta$, we solve:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\theta \\in \\Theta} \\min_M \\mathbb{E} \\left[M h(X,\\theta)\\right]\n",
    "\\end{equation*}\n",
    "subject to\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}\\left[  M f(X, \\theta) \\right] = 0, \\cr\n",
    "&\\mathbb{E} \\left( M \\right)  = 1, \\cr\n",
    "&\\mathbb{E} \\left( M \\log M \\right) \\le \\kappa\n",
    "\\end{align*}  \n",
    "\n",
    "This problem has important special cases that interest us.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Let $h(x,\\theta)$ be plus or minus  indicator function for an event of interest independent of $\\theta$.\n",
    "By solving two problems we infer lower and upper bounds on probabilities of events of interest.  (minus the max of the negative of a function gives the min.)\n",
    "\n",
    "\n",
    "- Partition $\\theta = (\\theta_1, \\theta_2)$ where $\\theta_1$ is the scalar parameter of interest.  Let \n",
    "$h(x,\\theta) = \\pm  \\theta_1$.    By solving the two problems, we can construct upper and lower bounds on the parameter\n",
    "$\\theta_1.$\n",
    "\n",
    "\n",
    "- Condition on $\\theta_2$.  We repeat the previous problem and build the constraint into the construction of the  set $\\Theta$.  Perform for alternative $\\theta_2$'s and trace out a boundary of a set of admissible parameters.\n",
    "\n",
    "\n",
    "## Dual problem\n",
    "\n",
    "\n",
    "For computational purposes, we solve the dual problem after minimizing over $M$.  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{\\theta \\in \\Theta}\n",
    "\\max_{\\xi \\ge 0, \\lambda}    - \\xi \\log {\\mathbb E} \\left[ \\exp\\left( - {\\frac 1 {\\xi}} \\left[ h(X, \\theta) + \\lambda \\cdot f(X, \\theta) \\right] \\right)\\right]  -  \\xi \\kappa  \n",
    "\\end{equation*}\n",
    "\n",
    "$\\lambda$ and $\\xi$ are multipliers on the moment condition and relative entropy constraints.  \n",
    "\n",
    "## Lower bound on relative entropy\n",
    "\n",
    "Under misspecification, there is lower bound on $\\kappa$.  We compute this by solving:\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\underline \\kappa} \\doteq  \\min_{\\theta \\in \\Theta} \\min_M\\mathbb{E}(M \\log M)\n",
    "\\end{equation*}\n",
    "subject to\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}\\left[  M f(X, \\theta) \\right] = 0, \\cr\n",
    "&\\mathbb{E} \\left( M \\right) = 1 .\n",
    "\\end{align*}\n",
    "\n",
    "Again this can be solved in a straightforward manner using a dual counterpart:\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\underline \\kappa} = \\min_{\\theta \\in \\Theta} \\max_{\\lambda} -\\log \\mathbb{E}\\exp[-\\lambda \\cdot f(X,\\theta)]\n",
    "\\end{equation*}\n",
    "\n",
    "##### Han please organize the Python codes using this structure.\n",
    "\n",
    "\n",
    "LPH stopped here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problem when $h = h(\\theta)$\n",
    "When $h$ does not depend on $X$, we can take it out of the expectation. Then the dual problem becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\min_{\\theta \\in \\Theta}}  h(\\theta)+ \n",
    "\\max_{\\xi \\ge 0} \\max_{\\lambda}  \\left[- \\xi \\log {\\mathbb E} \\exp\\left( - {\\frac 1 {\\xi}} \\left[\\lambda \\cdot f(X, \\theta) \\right] \\right)  -  \\xi \\kappa  \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Fix $\\xi$ and maximize over $\\lambda$, we have\n",
    "\\begin{equation*}\n",
    "\\min_{\\theta \\in \\Theta}  h(\\theta)+ \n",
    "\\max_{\\xi \\ge 0} \\xi\\left(\\kappa(\\theta)-\\kappa\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "The primal form of this problem is\n",
    "$$\\min_{\\theta \\in \\Theta} h(\\theta)$$\n",
    "*subject to*\n",
    "$$\\kappa(\\theta) \\leq \\kappa$$\n",
    "\n",
    "This form offers us computational advantages since computing relative entropy discrepancy is typically faster than computing the $\\lambda,\\xi$-maximization problem in the original dual problem.\n",
    "\n",
    "## Python solver\n",
    "\n",
    "The following Python class is designed to\n",
    "\n",
    "- calculate the relative entropy discrepancy at any given $\\theta$;\n",
    "\n",
    "\n",
    "- find the lower bound on relative entropy;\n",
    "\n",
    "\n",
    "- solve the basic problem with an arbitrary choice $h(X,\\theta)$.\n",
    "\n",
    "Examples come after the codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit,float64\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numba accelerators\n",
    "@jit\n",
    "def objective_λ_numba(f,λ):\n",
    "    # use \"max trick\"\n",
    "    x = f@λ\n",
    "    a = x.max()\n",
    "    return np.log(np.sum(np.exp(x-a)))+a\n",
    "\n",
    "@jit\n",
    "def objective_gradient_λ_numba(f,λ):\n",
    "    temp1 = f@λ\n",
    "    temp2 = f*(np.exp(temp1.reshape((len(temp1),1)))/np.mean(np.exp(temp1)))\n",
    "    temp3 = np.empty(temp2.shape[1])\n",
    "    for i in range(temp2.shape[1]):\n",
    "        temp3[i] = np.mean(temp2[:,i])\n",
    "    return temp3\n",
    "\n",
    "@jit\n",
    "def objective_ξ_λ_numba(ξ_λ,f,h,κ,lower):\n",
    "    ξ = ξ_λ[0]\n",
    "    λ = ξ_λ[1:]\n",
    "    temp1 = f@λ\n",
    "    temp2 = -np.log(f.shape[0])+κ\n",
    "    if lower == True:\n",
    "        # use \"max trick\"\n",
    "        x = -(temp1+h)/ξ\n",
    "        a = x.max()\n",
    "    else:\n",
    "        # use \"max trick\"\n",
    "        x = -(temp1-h)/ξ\n",
    "        a = x.max()\n",
    "    return (np.log(np.sum(np.exp(x-a)))+a+temp2)*ξ\n",
    "\n",
    "\n",
    "# Define class\n",
    "class Solver:\n",
    "    def __init__(self,X,functionf):\n",
    "        # Load data X and function f\n",
    "        self.X = X\n",
    "        self.functionf = functionf\n",
    "        \n",
    "        \n",
    "    # Define the objective function of the maximization problem when calculating relative entropy\n",
    "    def objective_λ(self,λ):\n",
    "        return objective_λ_numba(self.f,λ)\n",
    "        \n",
    "    \n",
    "    # Define the gradient of the objective function\n",
    "    def objective_gradient_λ(self,λ):\n",
    "        return objective_gradient_λ_numba(self.f,λ)\n",
    "    \n",
    "    \n",
    "    # Maximize over λ\n",
    "    def cal_divergence(self,\n",
    "                       θ,\n",
    "                       tol=1.0e-10,\n",
    "                       maxiter=200):\n",
    "        # Update f(X,θ)\n",
    "        self.f = self.functionf(self.X,θ)\n",
    "        \n",
    "        # Create an initial point for λ\n",
    "        initial_point = np.ones(self.f.shape[1])\n",
    "        \n",
    "        # Try L-BFGS-B, BFGS, CG sequentially\n",
    "        # Switch the method if algorithm does not converge\n",
    "        for method in ['L-BFGS-B','BFGS','CG']:\n",
    "            # use SciPy solver\n",
    "            model = minimize(self.objective_λ, \n",
    "                             initial_point, \n",
    "                             method=method,\n",
    "                             jac=self.objective_gradient_λ,\n",
    "                             tol=tol,\n",
    "                             options={'maxiter': maxiter})\n",
    "            \n",
    "            # Check if algorithm converges\n",
    "            if model.success == True:\n",
    "                fun_value = -(model.fun-np.log(self.f.shape[0]))\n",
    "                break\n",
    "            else:\n",
    "                fun_value = np.nan\n",
    "            \n",
    "        # Save results\n",
    "        result = {'result':fun_value,\n",
    "                'success':model.success,\n",
    "                'message':model.message,\n",
    "                'nit':model.nit}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    # Define the objective function of the ξ,λ-maximization problem\n",
    "    # Take negative sign to solve the ξ,λ-minimization problem\n",
    "    def objective_ξ_λ(self,ξ_λ):\n",
    "        return objective_ξ_λ_numba(ξ_λ,self.f,self.h,self.κ,self.lower)\n",
    "    \n",
    "        \n",
    "        \n",
    "    # Maximize the objective over λ and ξ\n",
    "    # Note: here κ is \\kappa\n",
    "    def maximize_ξ_λ(self,\n",
    "                        κ,\n",
    "                        θ,\n",
    "                        functionh,\n",
    "                        lower=True,\n",
    "                        tol=1.0e-10,\n",
    "                        maxiter=200):\n",
    "        # Update parameters and function values\n",
    "        self.κ = κ\n",
    "        self.f = self.functionf(self.X,θ)\n",
    "        self.lower = lower\n",
    "        self.h = functionh(self.X,θ)\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "        \n",
    "        # Check if the input θ satisfies the constraint\n",
    "        div_res = self.cal_divergence(θ,tol=self.tol,maxiter=self.maxiter)\n",
    "        \n",
    "        if self.κ - div_res['result'] < 0:\n",
    "            result = {'result':np.nan,\n",
    "                   'success':False,\n",
    "                   'message':'θ=' + str(θ) + ' is not feasible when κ=' + str(κ) + '...',\n",
    "                   'nit':0}\n",
    "        else:\n",
    "            # Create an initial point\n",
    "            initial_point = np.ones(self.f.shape[1]+1) \n",
    "            \n",
    "            # Create bounds for the parameters\n",
    "            bounds = []\n",
    "            for i in range(self.f.shape[1]+1):\n",
    "                if i==0:\n",
    "                    bounds.append((0,None))\n",
    "                else:\n",
    "                    bounds.append((None,None))\n",
    "\n",
    "            # Switch method if algorithm does not converge\n",
    "            for method in ['L-BFGS-B','SLSQP']:\n",
    "                model = minimize(self.objective_ξ_λ, \n",
    "                                 initial_point, \n",
    "                                 method=method,\n",
    "                                 tol=tol,\n",
    "                                 bounds=bounds,\n",
    "                                 options={'maxiter':maxiter})\n",
    "\n",
    "                # Check if algorithm converges\n",
    "                if model.success == True:\n",
    "                    fun_value = -model.fun\n",
    "                    break\n",
    "                else:\n",
    "                    fun_value = np.nan\n",
    "                          \n",
    "            # Save optimization status\n",
    "            result = {'result':fun_value,\n",
    "                   'success':model.success,\n",
    "                   'message':model.message,\n",
    "                   'nit':model.nit}\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def divergence_lower_bound(self,\n",
    "                               θ_bounds,\n",
    "                               grid_size,\n",
    "                               tol=1.0e-10,\n",
    "                               maxiter=200):\n",
    "        # Generate coordinate vectors: size*(dim of θ)\n",
    "        θ_vec = np.array([np.linspace(θ_bounds[i][0],θ_bounds[i][1],grid_size) for i in range(len(θ_bounds))])\n",
    "        # Generate coordinate meshgrid, size^(dim of θ)\n",
    "        θ_grid = np.array(list(itertools.product(*θ_vec)))\n",
    "        \n",
    "        # Solve for each pair of θ\n",
    "        result = []\n",
    "        for θ in θ_grid:\n",
    "            result.append(self.cal_divergence(θ,tol,maxiter))\n",
    "        \n",
    "        # Get divergence grid\n",
    "        div_grid = np.array([result[i]['result'] for i in range(len(result))])\n",
    "        # Find the minimal div\n",
    "        div_min = np.nanmin(div_grid)\n",
    "        \n",
    "        # Find θ that minimizes div\n",
    "        div_min_arg = np.nanargmin(div_grid)\n",
    "        θ_min = θ_grid[div_min_arg]\n",
    "        \n",
    "        return {'div_min':div_min,\n",
    "               'θ':θ_min}\n",
    "    \n",
    "        \n",
    "    def expectation_bound(self,\n",
    "                          κ,\n",
    "                          functionh,\n",
    "                          functionh_depends_on_x,\n",
    "                          θ_independent,\n",
    "                          θ_dependent,\n",
    "                          grid_size,\n",
    "                          tol=1.0e-10,\n",
    "                          maxiter=200):\n",
    "        # Generate coordinate vectors: size*(dim of θ_1)\n",
    "        θ_vec = np.array([np.linspace(θ_dependent['bounds'][i][0],θ_dependent['bounds'][i][1],grid_size) \\\n",
    "                          for i in range(len(θ_dependent['bounds']))])\n",
    "        # Generate coordinate meshgrid, size^(dim of θ_1)\n",
    "        θ_grid = np.array(list(itertools.product(*θ_vec)))\n",
    "        \n",
    "        # Add independent θ to the meshgrid\n",
    "        if θ_independent is not None:\n",
    "            for i in range(len(θ_independent['positions'])):\n",
    "                θ_grid = np.insert(θ_grid,θ_independent['positions'][i]-1,θ_independent['values'][i],axis=1)\n",
    "\n",
    "        # Solve for each pair of θ\n",
    "        result = []\n",
    "        for θ in θ_grid:\n",
    "            result.append(self.cal_divergence(θ,tol,maxiter))\n",
    "\n",
    "        # Get divergence grid\n",
    "        div_grid = np.array([result[i]['result'] for i in range(len(result))])\n",
    "        \n",
    "        # Find feasible θ grid and corresponding divergence grid\n",
    "        div_grid[div_grid>κ] = np.nan\n",
    "        θ_grid_feasible = θ_grid[~np.isnan(div_grid)]\n",
    "        div_grid_feasible = div_grid[~np.isnan(div_grid)]\n",
    "        \n",
    "        # Check if κ is reasonable\n",
    "        if len(θ_grid_feasible) == 0:\n",
    "            print('No feasible region found...please increase κ...')\n",
    "            return np.nan\n",
    "        \n",
    "        if functionh_depends_on_x == True:\n",
    "            # Solve for each pair of θ\n",
    "            result_exp = []\n",
    "            for θ in θ_grid_feasible:\n",
    "                result_lower = self.maximize_ξ_λ(κ,θ,functionh,True,tol,maxiter)\n",
    "                result_upper = self.maximize_ξ_λ(κ,θ,functionh,False,tol,maxiter)\n",
    "                result_exp.append([result_lower,result_upper])\n",
    "\n",
    "            exp_low_grid = np.array([result_exp[i][0]['result'] for i in range(len(result_exp))])\n",
    "            exp_up_grid = np.array([result_exp[i][1]['result'] for i in range(len(result_exp))])\n",
    "\n",
    "            # Find the lower bound of the expectation\n",
    "            exp_low = np.nanmin(exp_low_grid)\n",
    "            # Find the upper bound of the expectation\n",
    "            exp_up = -np.nanmin(exp_up_grid)\n",
    "\n",
    "            # Find θ that minimizes the expectation\n",
    "            exp_low_arg = np.nanargmin(exp_low_grid)\n",
    "            θ_low = θ_grid_feasible[exp_low_arg]\n",
    "            # Find θ that maximizes the expectation\n",
    "            exp_up_arg = np.nanargmin(exp_up_grid)\n",
    "            θ_up = θ_grid_feasible[exp_up_arg]\n",
    "\n",
    "        else:\n",
    "            # Create meshgrid for h(θ)\n",
    "            h_θ_grid = np.array([functionh(X=None,θ=θ) for θ in θ_grid_feasible])\n",
    "            # Find the smallest h(θ_dep) from the feasible region\n",
    "            exp_low = np.nanmin(h_θ_grid)\n",
    "            # Find the largest h(θ_dep) from the feasible region\n",
    "            exp_up = np.nanmax(h_θ_grid)\n",
    "            \n",
    "            # Find θ that minimizes the expectation\n",
    "            exp_low_arg = np.nanargmin(h_θ_grid)\n",
    "            θ_low = θ_grid_feasible[exp_low_arg]\n",
    "            # Find θ that maximizes the expectation\n",
    "            exp_up_arg = np.nanargmax(h_θ_grid)\n",
    "            θ_up = θ_grid_feasible[exp_up_arg]\n",
    "        \n",
    "        return {'exp_low':exp_low,\n",
    "                'exp_up':exp_up,\n",
    "                'θ_low':θ_low,\n",
    "                'θ_up':θ_up}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. calculate relative entropy discrepancy\n",
    "\n",
    "Suppose $X$ include five variables: $G_{t,t+1},R_{f,t+1},R_{m,t+1}^e,R_{SMB,t+1}^e,R_{HML,t+1}^e$.\n",
    "\n",
    "Let $f(X,\\theta)$ be\n",
    "\n",
    "$$ f(X_{t+1},\\theta) = \n",
    "\\left[ \\begin{array}{c} \n",
    "\\delta \\left( G_{t,t+1} \\right)^{-\\gamma} R_{f,t+1}  - 1  \\\\ \n",
    "\\delta \\left( G_{t,t+1} \\right)^{-\\gamma} R_{m,t+1}^e \\\\\n",
    "\\delta \\left( G_{t,t+1} \\right)^{-\\gamma} R_{SMB,t+1}^e \\\\\n",
    "\\delta \\left( G_{t,t+1} \\right)^{-\\gamma} R_{HML,t+1}^e\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "The relative entropy discrepancy is given by\n",
    "$$\\kappa(\\theta) = \\displaystyle \\max_{\\lambda} -\\log \\mathbb{E}\\exp[-\\lambda \\cdot f(X,\\theta)]$$\n",
    "\n",
    "Since the objective is concave in $\\lambda$, the solver uses convex optimization algorithm [L-BFGS-B](http://sepwww.stanford.edu/data/media/public/docs/sep117/antoine1/paper_html/node6.html) (or [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm), [CG](https://en.wikipedia.org/wiki/Conjugate_gradient_method) if it does not converge) to solve $\\kappa(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "data = pd.read_csv('CleanDataQuarterly.csv',index_col=0)\n",
    "X = np.array(data)\n",
    "\n",
    "# Define f(X,θ), using numba accelerator\n",
    "@jit(nopython=True)\n",
    "def functionf(X,θ):\n",
    "    vec = np.power(X[:,0:1],-θ[1])\n",
    "    M1 = X[:,1:] * vec\n",
    "    Y = M1 * θ[0] - np.array([1,0,0,0])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative entropy divergence is: 0.5856825399020096.\n",
      "--- 1.5409 seconds ---\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() # Count time\n",
    "solver = Solver(X,functionf) # Initialize solver\n",
    "result = solver.cal_divergence(θ=np.array([0.95,15.0]), # θ at which κ(θ) being calculated\n",
    "                               tol=1.0e-5, # Tolerance level\n",
    "                               maxiter=200) # Maximal # of iterations\n",
    "\n",
    "# Print results\n",
    "print('The relative entropy divergence is: '+str(result['result'])+'.')\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. find lower bound on relative entropy\n",
    "\n",
    "The lower bound can be solved by\n",
    "\n",
    "$$\\underline{\\kappa} = \\displaystyle \\min_{\\theta \\in \\Theta}\\kappa(\\theta)$$\n",
    "\n",
    "$\\kappa(\\theta)$ is not necessarily convex in $\\theta$. The solver uses Grid Search approach to find $\\underline{\\kappa}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimal relative entropy is: 0.15264768751860647, achieved at θ=[1. 5.]\n",
      "--- 3.4776 seconds ---\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() # Count time\n",
    "solver = Solver(X,functionf) # Initialize solver\n",
    "result = solver.divergence_lower_bound(θ_bounds=[(0.95,1.0),(5.0,35.0)], # Bounds of θ\n",
    "                                       grid_size=100, # Specify # of grid points in each parameter dimension\n",
    "                                       tol=1.0e-5, # Tolerance level\n",
    "                                       maxiter=200) # Maximal # of iterations\n",
    "\n",
    "# Print results\n",
    "print('The minimal relative entropy is: '+str(result['div_min']) +', achieved at θ='+str(result['θ']))\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3. find lower/upper bounds on expectation\n",
    "\n",
    "The lower bound on expectation of $h(X,\\theta)$ is given by\n",
    "\\begin{equation*}\n",
    "{\\min_{\\theta \\in \\Theta}} \n",
    "\\max_{\\xi \\ge 0, \\lambda}    - \\xi \\log {\\mathbb E} \\left[ \\exp\\left( - {\\frac 1 {\\xi}} \\left[ h(X, \\theta) + \\lambda \\cdot f(X, \\theta) \\right] \\right)\\right]  -  \\xi \\kappa  \n",
    "\\end{equation*}\n",
    "\n",
    "Since the objective is concave in $(\\xi, \\lambda)$, the solver will first use convex optimization algorithms [L-BFGS-B](http://sepwww.stanford.edu/data/media/public/docs/sep117/antoine1/paper_html/node6.html) or [SLSQP](http://degenerateconic.com/slsqp/) to solve\n",
    "\n",
    "\\begin{equation*}\n",
    "\\max_{\\xi \\ge 0, \\lambda}    - \\xi \\log {\\mathbb E} \\left[ \\exp\\left( - {\\frac 1 {\\xi}} \\left[ h(X, \\theta) + \\lambda \\cdot f(X, \\theta) \\right] \\right)\\right]  -  \\xi \\kappa  \n",
    "\\end{equation*}\n",
    "\n",
    "for each pair of $\\theta \\in \\Theta$. Then it uses Grid Search approach to minimize over $\\theta$. The upper bound problem can be solved in the same way by replacing $h$ with $-h$.\n",
    "\n",
    "The choice of $h(X,\\theta)$ can be arbitrary. Below we show three important special cases.\n",
    "\n",
    "#### Case 1. $h(X,\\theta)$ is an indicator function of $X$\n",
    "\n",
    "$$h(X_{t+1}) = \\mathbb{1} \\{R_{m,t+1}^e>0\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define h(X,θ), using numba accelerator\n",
    "@jit\n",
    "def functionh(X,θ):\n",
    "    vec = X[:,2].copy()\n",
    "    vec[vec>0] = 1\n",
    "    vec[vec<0] = 0\n",
    "    return vec\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "time_start = time.time() # Count time\n",
    "model = Solver(X,functionf) # Initialize solver\n",
    "result = model.expectation_bound(κ=1.0, # Threshold κ\n",
    "                                 functionh=functionh, # Function defined above\n",
    "                                 functionh_depends_on_x=True, # True if functionh depends on x\n",
    "                                 θ_independent=None, # Not condition on any θ_i\n",
    "                                 θ_dependent={'positions':[1,2], # Target θ=(θ_1,θ_2)\n",
    "                                             'bounds':[(0.95,1.0),(5.0,35.0)]}, # Bounds of target θ \n",
    "                                 grid_size=100, # Specify grid size\n",
    "                                 tol=1.0e-10, # Tolerance level\n",
    "                                 maxiter=200) # Maximal # of iterations\n",
    "\n",
    "# Print results\n",
    "print('The lower bound for the expectation is: '+str(result['exp_low']) +', achieved at θ='+str(result['θ_low']))\n",
    "print('The upper bound for the expectation is: '+str(result['exp_up']) + ', achieved at θ='+str(result['θ_up']))\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. $h(X,\\theta) = \\theta_1$\n",
    "\n",
    "$$h(X,\\theta) = \\theta_1$$\n",
    "\n",
    "where $\\theta_1$ is a scalar parameter of our interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define h(X,θ), using numba accelerator\n",
    "@jit\n",
    "def functionh(X,θ):\n",
    "    return θ[0]\n",
    "\n",
    "time_start = time.time() # Count time\n",
    "model = Solver(X,functionf) # Initialize solver\n",
    "result = model.expectation_bound(κ=1.0, # Threshold κ\n",
    "                                 functionh=functionh, # Function defined above\n",
    "                                 functionh_depends_on_x=False, # True if functionh depends on x\n",
    "                                 θ_independent=None, # Not condition on any θ_i\n",
    "                                 θ_dependent={'positions':[1,2], # Target θ=(θ_1,θ_2)\n",
    "                                             'bounds':[(0.95,1.0),(5.0,35.0)]}, # Bounds of target θ \n",
    "                                 grid_size=100, # Specify grid size\n",
    "                                 tol=1.0e-5, # Tolerance level\n",
    "                                 maxiter=200) # Maximal # of iterations\n",
    "\n",
    "# Print results\n",
    "print('The lower bound for the expectation is: '+str(result['exp_low']) +', achieved at θ='+str(result['θ_low']))\n",
    "print('The upper bound for the expectation is: '+str(result['exp_up']) + ', achieved at θ='+str(result['θ_up']))\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3. $h(X,\\theta) = \\theta_1$, condition on $\\theta_2$\n",
    "\n",
    "$$h(X,\\theta) = \\theta_1$$\n",
    "\n",
    "where $\\theta_1$ is a scalar parameter of our interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define h(X,θ)=θ_1, using numba accelerator\n",
    "@jit\n",
    "def functionh(X,θ):\n",
    "    return θ[0]\n",
    "\n",
    "time_start = time.time() # Count time\n",
    "solver = Solver(X,functionf) # Initialize solver\n",
    "result = solver.expectation_bound(κ=1.0, # Threshold κ\n",
    "                                  functionh=functionh, # Function defined above\n",
    "                                  functionh_depends_on_x=False, # True if functionh depends on x\n",
    "                                  θ_independent={'positions':[2], # Condition on θ_2\n",
    "                                                 'values':[15.0]}, # Value of θ_2\n",
    "                                  θ_dependent={'positions':[1], # Target θ=(θ_1)\n",
    "                                               'bounds':[(0.95,1.0)]}, # Bounds of target θ\n",
    "                                  grid_size=100, # Specify grid size\n",
    "                                  tol=1.0e-5, # Tolerance level\n",
    "                                  maxiter=200) # Maximal # of iterations\n",
    "\n",
    "# Print results\n",
    "print('The lower bound for the expectation is: '+str(result['exp_low']) +', achieved at θ='+str(result['θ_low']))\n",
    "print('The upper bound for the expectation is: '+str(result['exp_up']) + ', achieved at θ='+str(result['θ_up']))\n",
    "print(\"--- %s seconds ---\" % (round(time.time()-time_start,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
